{
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code"
    },
    {
      "source": [
        "\nL1-regularized logistic regression\n==================================\n\nImplementation of L1-regularized logistic regression\nusing copt.\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "execution_count": null,
      "source": [
        "import numpy as np\nfrom sklearn.linear_model import logistic\nfrom copt import proximal_gradient\n\nn_samples, n_features = 100, 10\nX = np.random.randn(n_samples, n_features)\ny = np.random.randn(n_samples)\nalpha = 1.\n\n\ndef logloss(x):\n    return logistic._logistic_loss(x, X, y, 1.)\n\n\ndef fprime_logloss(x):\n    return logistic._logistic_loss_and_grad(x, X, y, 1.)[1]\n\n\ndef g_prox(x, step_size):\n    \"\"\"\n    prox of alpha * l1\n    \"\"\"\n    return np.fmax(x - step_size * alpha, 0) - \\\n        np.fmax(- x - step_size * alpha, 0)\n\n\nout = proximal_gradient(logloss, fprime_logloss, g_prox, np.zeros(n_features))\nprint('Solution', out)"
      ],
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "cell_type": "code"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  }
}